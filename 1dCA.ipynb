{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLSc4_9-5oAt",
        "outputId": "fc88306d-076f-4099-9957-ce6ebed2ded1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79arZ7cHpvMs",
        "outputId": "238d159a-8179-4d1f-c9d9-8b404d34ca69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/TransformerECA\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/TransformerECA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AcI6oKsAwPx"
      },
      "source": [
        "# CA generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sag6KgQ8BNAl"
      },
      "source": [
        "## Explore CA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7XLBpZKCigW"
      },
      "outputs": [],
      "source": [
        "! pip install cellpylib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gD_ltf8CtoO"
      },
      "outputs": [],
      "source": [
        "import cellpylib as cpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def generate_rule_array(r):\n",
        "    # Length of the binary rule array\n",
        "    rule_length = 2 ** (2 * r + 1)\n",
        "    # Generating a random rule as a binary array\n",
        "    rule_array = np.random.choice([0, 1], rule_length)\n",
        "    return rule_array\n",
        "\n",
        "r = 1\n",
        "size = 250\n",
        "T = 200\n",
        "\n",
        "cellular_automaton = cpl.init_random(size)\n",
        "rule_number = generate_rule_array(r)\n",
        "rule_number = [1,1,0,0,1,1,0,0]\n",
        "#rule_number = [0,1,1,0,1,1,0,0]\n",
        "\n",
        "# rule_number = np.random.randint(2**2**(2*r+1))\n",
        "# rule_number = 1349837757 # 1196030057 # 1110475378\n",
        "# print(bin(rule_number)[2:])\n",
        "\n",
        "# evolve the CA, setting r to 2, for a neighbourhood size of 5\n",
        "cellular_automaton = cpl.evolve(cellular_automaton, timesteps=T, memoize=True,\n",
        "                                apply_rule=lambda n, c, t: cpl.binary_rule(n, rule_number),\n",
        "                                r = r)\n",
        "\n",
        "scale = 20\n",
        "plt.figure(figsize=((int) (size/scale), (int) (T/scale)))\n",
        "plt.imshow(cellular_automaton, cmap='cividis', aspect='auto')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS62XJpKXfIM"
      },
      "outputs": [],
      "source": [
        "rule_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TULbkw2-BFWr"
      },
      "source": [
        "## CA Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud1PJ5ZmEOvh",
        "outputId": "3a69d91d-3568-4870-c09e-3e8f5f2c518d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: pip: command not found\n",
            "/bin/bash: line 1: pip: command not found\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets\n",
        "! pip install cellpylib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn80Hl16IRQT"
      },
      "outputs": [],
      "source": [
        "import cellpylib as cpl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def generate_ca_dataset(r, size, T, num_samples):\n",
        "    \"\"\"\n",
        "    Generate a dataset for 1D cellular automata.\n",
        "\n",
        "    Parameters:\n",
        "    r (int): The radius of the CA rule.\n",
        "    size (int): The number of cells in the CA.\n",
        "    T (int): The number of steps for the CA to evolve.\n",
        "    num_samples (int): The number of samples in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A pandas DataFrame containing the dataset.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    for _ in tqdm(range(num_samples), desc=\"Generating dataset\"):\n",
        "        # Generate the initial state and the rule number\n",
        "        initial_state = cpl.init_random(size)\n",
        "        #rule_number = np.random.randint(2 ** (2 ** (2 * r + 1)))\n",
        "        rule_number = np.random.choice([0, 1], size=(2 ** (2 * r + 1))).tolist()\n",
        "        #rule_number = [1,1,0,0,1,1,0,0] # rule 51\n",
        "        #rule_bin = format(rule_number, f'0{2 ** (2 * r + 1)}b')\n",
        "        rule_bin = ''.join(str(bit) for bit in rule_number)\n",
        "\n",
        "        # Evolve the CA\n",
        "        ca = cpl.evolve(initial_state, timesteps=T, memoize=True,\n",
        "                        apply_rule=lambda n, c, t: cpl.binary_rule(n, rule_number), r=r)\n",
        "\n",
        "        # Convert CA states to strings and add to dataset\n",
        "        states = [''.join(map(str, state)) for state in ca]\n",
        "        dataset.append([rule_bin, *states])\n",
        "\n",
        "    # Create DataFrame\n",
        "    columns = ['rule'] + [f't={t}' for t in range(T)]\n",
        "    return pd.DataFrame(dataset, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tihF3sgI8xZl"
      },
      "outputs": [],
      "source": [
        "def generate_ca_dataset_by_rules(r, size, T, num_samples, rules):\n",
        "    dataset = []\n",
        "    num_rules = len(rules)\n",
        "\n",
        "    for _ in tqdm(range(num_samples), desc=\"Generating dataset\"):\n",
        "        initial_state = cpl.init_random(size)\n",
        "        rule_number = np.random.choice(rules)  # Select from the specified set of rules\n",
        "        rule_bin = format(rule_number, f'0{2 ** (2 * r + 1)}b')\n",
        "\n",
        "        # Evolve the CA\n",
        "        ca = cpl.evolve(initial_state, timesteps=T, memoize=True,\n",
        "                        apply_rule=lambda n, c, t: cpl.binary_rule(n, rule_number), r=r)\n",
        "\n",
        "        # Convert CA states to strings and add to dataset\n",
        "        states = [''.join(map(str, state)) for state in ca]\n",
        "        dataset.append([rule_bin, *states])\n",
        "\n",
        "    # Create DataFrame\n",
        "    columns = ['rule'] + [f't={t}' for t in range(T)]\n",
        "    return pd.DataFrame(dataset, columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o776Wd-z8cRI"
      },
      "outputs": [],
      "source": [
        "def partition_rules(r, train_samples, test_samples):\n",
        "    \"\"\"\n",
        "    Uniformly samples a subset of rules from the entire space of possible rules for a given radius 'r'.\n",
        "    The number of rules sampled is limited to 'num_samples' and then partitioned for training/validation and testing.\n",
        "\n",
        "    Parameters:\n",
        "    - r (int): The radius of the CA rule, determining the rule space size.\n",
        "    - num_samples (int): The upper limit on the number of unique rules to consider.\n",
        "    - test_prop (float): Proportion of the rules to be used for testing.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Two numpy arrays containing rule numbers for training/validation and testing.\n",
        "    \"\"\"\n",
        "    # Calculate the total number of possible rules\n",
        "    total_rules = 2 ** (2 ** (2 * r + 1))\n",
        "\n",
        "    # Determine the number of unique rules to sample\n",
        "    num_rules = min((num_samples+test_samples), total_rules)\n",
        "\n",
        "    # Randomly sample unique rule numbers from the total rule space\n",
        "    sampled_rules = np.random.choice(total_rules, size=num_rules, replace=False)\n",
        "\n",
        "    # Calculate the number of rules to be used for testing\n",
        "    num_test_rules = test_samples #int(num_rules * test_prop)\n",
        "\n",
        "    # Split the sampled rules into test and training/validation rule sets\n",
        "    np.random.shuffle(sampled_rules)  # Ensure random distribution for partitioning\n",
        "    test_rules = sampled_rules[:num_test_rules]\n",
        "    train_val_rules = sampled_rules[num_test_rules:]\n",
        "\n",
        "    return train_val_rules, test_rules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV8t4jMfXble",
        "outputId": "8b307f94-7441-4ab6-94a9-66c669d1c6c0",
        "colab": {
          "referenced_widgets": [
            "aef81efee30747af869373d9fa85d337"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aef81efee30747af869373d9fa85d337",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating dataset:   0%|          | 0/1111111 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Constants\n",
        "r = 4\n",
        "size = 20\n",
        "T = 20\n",
        "num_samples = 1_111_111\n",
        "test_samples = 100_000\n",
        "\n",
        "# Get the partitions for rules\n",
        "#train_val_rules, test_rules = partition_rules(r, num_samples, test_samples)\n",
        "\n",
        "# Generate the dataset\n",
        "df = generate_ca_dataset(r, size, T, num_samples)\n",
        "#train_val_df = generate_ca_dataset_by_rules(r, size, T, len(train_val_rules), train_val_rules)\n",
        "#test_df = generate_ca_dataset_by_rules(r, size, T, len(test_rules), test_rules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO9FKxamXzg3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Directory for dataset files\n",
        "dataset_dir = '1dCA_r'+str(r)+'s'+str(size)+'T'+str(T)\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Splitting the dataset into train, validation, and test sets\n",
        "# Mixed Train\\Val\\Test Adjust the proportions as per your requirements\n",
        "train_df = df.sample(frac=0.9, random_state=123) # 80% for training\n",
        "test_df = df.drop(train_df.index)\n",
        "validation_df = test_df.sample(frac=0.2, random_state=123) # 10% for validation\n",
        "test_df = test_df.drop(validation_df.index) # 10% for testing\n",
        "\n",
        "# Mixed Train\\Val and Separate Test. Split train and validation\n",
        "#train_df = train_val_df.sample(frac=0.95, random_state=123)  # Adjusted for 90% of train_val\n",
        "#validation_df = train_val_df.drop(train_df.index)\n",
        "\n",
        "# Saving the splits in JSON format\n",
        "train_df.to_json(os.path.join(dataset_dir, 'train.json'), orient='records', lines=True)\n",
        "validation_df.to_json(os.path.join(dataset_dir, 'validation.json'), orient='records', lines=True)\n",
        "test_df.to_json(os.path.join(dataset_dir, 'test.json'), orient='records', lines=True)\n",
        "\n",
        "# Create a README file with dataset description\n",
        "readme_text = f\"\"\"\n",
        "# 1D Cellular Automata Dataset\n",
        "\n",
        "## Structure\n",
        "\n",
        "- `rule`: The rule number in binary format\n",
        "- `t=0`, `t=1`, ..., `t=T`: The states of the CA at each timestep\n",
        "\n",
        "## Splits\n",
        "\n",
        "- Training: 80%\n",
        "- Validation: 10%\n",
        "- Test: 10%\n",
        "\n",
        "## Parameters:\n",
        "\n",
        "| Parameter         | Description                                |\n",
        "|-------------------|--------------------------------------------|\n",
        "| r (int): `{r}`    | The radius of the CA rule.                 |\n",
        "| size (int): `{size}` | The number of cells in the CA.             |\n",
        "| T (int): `{T}`    | The number of steps for the CA to evolve.  |\n",
        "| num_samples (int): `{num_samples}` | The number of samples in the dataset. |\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(dataset_dir, 'README.md'), 'w') as file:\n",
        "    file.write(readme_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC2348rvGWMO"
      },
      "outputs": [],
      "source": [
        "# Load the dataset using Hugging Face's load_dataset function\n",
        "data_files = {\n",
        "    'train': os.path.join(dataset_dir, 'train.json'),\n",
        "    'validation': os.path.join(dataset_dir, 'validation.json'),\n",
        "    'test': os.path.join(dataset_dir, 'test.json')\n",
        "}\n",
        "\n",
        "dataset = load_dataset('json', data_files=data_files, split='train+validation+test')\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whjbe9Q7HkzJ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTZzpqxXHohl",
        "outputId": "1d3aaece-f45b-40c5-f1b9-19eff1b81775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1dCA_GPU0_RuleTask.ipynb\t   1dCA_r2s84T6\n",
            " 1dCA_GPU0_Transformer.ipynb\t   1dCA_r3s24T20\n",
            " 1dCA_GPU1_Transformer.ipynb\t   babilong\n",
            " 1dCA_GPU2_Transformer.ipynb\t   babilong_evals\n",
            " 1dCA_GPU3_Transformer.ipynb\t   docs_docs.json\n",
            " 1dCA_GPU4_Transformer.ipynb\t   eval_Gemma_babilong.ipynb\n",
            " 1dCA_GPU6_RuleTask.ipynb\t  'eval_GPT_4x_BABILong(NeurIPS24).ipynb'\n",
            " 1dCA_GPU7_RuleTask.ipynb\t   eval_Llama3_babilong.ipynb\n",
            " 1dCA.ipynb\t\t\t   eval_Llama-3.ipynb\n",
            "'1dCA[M]_GPU0_Transformer.ipynb'   eval_llm_babilong.ipynb\n",
            "'1dCA[M]_GPU1_Transformer.ipynb'   eval_RAG_Llama3_babilong-Copy1.ipynb\n",
            "'1dCA[M]_GPU5_RuleTask.ipynb'\t   eval_RAG_Llama3_babilong-Copy2.ipynb\n",
            " 1dCA_r2s20T20\t\t\t   gemma-2B-10M\n",
            " 1dCA_r2s24T20\t\t\t   lost+found\n"
          ]
        }
      ],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN6Arcz7UdM5",
        "outputId": "4ef8a020-d71d-4207-a557-f758a343e43b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: zip: command not found\n"
          ]
        }
      ],
      "source": [
        "! zip -r 1dCA_r4s20T20.zip  1dCA_r4s20T20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVFrQqAeUhFB"
      },
      "outputs": [],
      "source": [
        "!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMEOZ_B7Uml_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "py310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}